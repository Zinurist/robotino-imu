{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import samples as ls\n",
    "import math\n",
    "import model_types\n",
    "\n",
    "#new models to test: 10105, (20)*5\n",
    "\n",
    "step_counter = 0\n",
    "load_rnn = False\n",
    "load_samples = True\n",
    "model_num = 10105\n",
    "overlap_step = 1\n",
    "\n",
    "if model_num == 0: #SIMPLE TRUNC, adam\n",
    "    #number of dimensions used and steps when unrolling\n",
    "    input_dim = 6\n",
    "    input_steps = None\n",
    "    unroll_steps = 50\n",
    "    use_lstm = False\n",
    "    #if false, only label of the last output is compared\n",
    "    #else label at each timestep is compared\n",
    "    use_labels_data = False\n",
    "    use_ctc = False\n",
    "    hidden_layers = [50]\n",
    "    labels,sample_ids,filename = model_types.get_labels(0)\n",
    "elif model_num == 100: #SIMPLE TRUNC, adam\n",
    "    input_dim = 6\n",
    "    input_steps = None\n",
    "    unroll_steps = 30\n",
    "    overlap_step = 15\n",
    "    use_lstm = True\n",
    "    use_labels_data = False\n",
    "    use_ctc = False\n",
    "    hidden_layers = [20]\n",
    "    labels,sample_ids,filename = model_types.get_labels(0)\n",
    "elif model_num == 10100: #SIMPLE TRUNC, adam\n",
    "    input_dim = 6\n",
    "    input_steps = None\n",
    "    unroll_steps = 20\n",
    "    use_lstm = False\n",
    "    use_labels_data = False\n",
    "    use_ctc = False\n",
    "    hidden_layers = [50]\n",
    "    labels,sample_ids,filename = model_types.get_labels(0)\n",
    "elif model_num == 7: #SIMPLE TRUNC, adam\n",
    "    input_dim = 6\n",
    "    input_steps = None\n",
    "    unroll_steps = 30\n",
    "    use_lstm = False\n",
    "    use_labels_data = False\n",
    "    use_ctc = False\n",
    "    hidden_layers = [50]\n",
    "    labels,sample_ids,filename = model_types.get_labels(2)\n",
    "elif model_num == 107: #SIMPLE TRUNC, adam\n",
    "    input_dim = 6\n",
    "    input_steps = None\n",
    "    unroll_steps = 20\n",
    "    use_lstm = False\n",
    "    use_labels_data = False\n",
    "    use_ctc = False\n",
    "    hidden_layers = [50]\n",
    "    labels,sample_ids,filename = model_types.get_labels(2)\n",
    "elif model_num == 10107: #SIMPLE TRUNC, adam\n",
    "    input_dim = 6\n",
    "    input_steps = 50\n",
    "    unroll_steps = 50\n",
    "    overlap_step = 25\n",
    "    use_lstm = False\n",
    "    use_labels_data = True\n",
    "    use_ctc = False\n",
    "    hidden_layers = [50]\n",
    "    labels,sample_ids,filename = model_types.get_labels(2)\n",
    "elif model_num == 1010107: #SIMPLE TRUNC, adam\n",
    "    input_dim = 6\n",
    "    input_steps = 30\n",
    "    unroll_steps = 30\n",
    "    overlap_step = 15\n",
    "    use_lstm = True\n",
    "    use_labels_data = True\n",
    "    use_ctc = False\n",
    "    hidden_layers = [50]\n",
    "    labels,sample_ids,filename = model_types.get_labels(2)\n",
    "elif model_num == 5: #SIMPLE TRUNC, adam\n",
    "    input_dim = 6\n",
    "    input_steps = None\n",
    "    unroll_steps = 50\n",
    "    use_lstm = False\n",
    "    use_labels_data = False\n",
    "    use_ctc = False\n",
    "    hidden_layers = [50]\n",
    "    labels,sample_ids,filename = model_types.get_labels(1)\n",
    "elif model_num == 105: #SIMPLE TRUNC, adam\n",
    "    input_dim = 6\n",
    "    input_steps = None\n",
    "    unroll_steps = 50\n",
    "    use_lstm = True\n",
    "    use_labels_data = False\n",
    "    use_ctc = False\n",
    "    hidden_layers = [50]\n",
    "    labels,sample_ids,filename = model_types.get_labels(1)\n",
    "elif model_num == 10105: #SIMPLE TRUNC, adam\n",
    "    input_dim = 6\n",
    "    input_steps = None\n",
    "    unroll_steps = 100\n",
    "    overlap_step = 50\n",
    "    use_lstm = False\n",
    "    use_labels_data = False\n",
    "    use_ctc = False\n",
    "    hidden_layers = [50]\n",
    "    labels,sample_ids,filename = model_types.get_labels(1)\n",
    "elif model_num == 1010105: #SIMPLE TRUNC, adam\n",
    "    input_dim = 6\n",
    "    input_steps = None\n",
    "    unroll_steps = 10\n",
    "    use_lstm = False\n",
    "    use_labels_data = False\n",
    "    use_ctc = False\n",
    "    hidden_layers = [50]\n",
    "    labels,sample_ids,filename = model_types.get_labels(1)\n",
    "elif model_num == 101010105: #SIMPLE TRUNC, adam\n",
    "    input_dim = 6\n",
    "    input_steps = None\n",
    "    unroll_steps = 20\n",
    "    overlap_step = 10\n",
    "    use_lstm = False\n",
    "    use_labels_data = False\n",
    "    use_ctc = False\n",
    "    hidden_layers = [20]\n",
    "    labels,sample_ids,filename = model_types.get_labels(1)\n",
    "elif model_num == 205: #SIMPLE TRUNC, adam\n",
    "    input_dim = 6\n",
    "    input_steps = 20\n",
    "    unroll_steps = 20\n",
    "    use_lstm = False\n",
    "    use_labels_data = True\n",
    "    use_ctc = False\n",
    "    hidden_layers = [50]\n",
    "    labels,sample_ids,filename = model_types.get_labels(1)\n",
    "elif model_num == 20205: #SIMPLE TRUNC, adam\n",
    "    input_dim = 6\n",
    "    input_steps = 50\n",
    "    unroll_steps = 50\n",
    "    overlap_step = 25\n",
    "    use_lstm = False\n",
    "    use_labels_data = True\n",
    "    use_ctc = False\n",
    "    hidden_layers = [50]\n",
    "    labels,sample_ids,filename = model_types.get_labels(1)\n",
    "elif model_num == 2020205: #SIMPLE TRUNC, adam\n",
    "    input_dim = 6\n",
    "    input_steps = 100\n",
    "    unroll_steps = 100\n",
    "    overlap_step = 50\n",
    "    use_lstm = True\n",
    "    use_labels_data = True\n",
    "    use_ctc = False\n",
    "    hidden_layers = [50]\n",
    "    labels,sample_ids,filename = model_types.get_labels(1)\n",
    "elif model_num == 10:#unused\n",
    "    input_dim = 6\n",
    "    input_steps = None\n",
    "    unroll_steps = None\n",
    "    use_lstm = False\n",
    "    use_labels_data = False\n",
    "    use_ctc = True\n",
    "    hidden_layers = [50]\n",
    "    labels,sample_ids,filename = model_types.get_labels(3)\n",
    "else:\n",
    "    raise ValueError('Unknown model')\n",
    "    \n",
    "classes = len(labels)\n",
    "if use_ctc: classes += 1\n",
    "\n",
    "#percentage used for test data\n",
    "test_rate = 0.1\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "#some assertion which might catch wrong config (not guaranteed)\n",
    "if use_labels_data:\n",
    "    assert input_steps is not None\n",
    "if use_ctc:\n",
    "    assert input_steps is None\n",
    "    assert not use_labels_data\n",
    "    assert 'straight' not in labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_samples:\n",
    "    samples = ls.load(filename)\n",
    "else:\n",
    "    samples = ls.Samples(labels)\n",
    "    \n",
    "    for label in labels:\n",
    "        samples.load_samples(label = label, sample_ids = sample_ids)\n",
    "\n",
    "    samples.convert_to_input(size = None, accel='xyz', gyro='xyz', compass='')\n",
    "\n",
    "samples.unroll(unroll_steps, overlap_step)\n",
    "if not use_ctc: samples.convert_to_onehot()\n",
    "samples.split_test(test_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, input_steps, input_dim], name='x')\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "Ws = []\n",
    "for i in range(len(hidden_layers)-1):\n",
    "    W = tf.Variable(tf.random_normal([hidden_layers[i], hidden_layers[i+1]]))\n",
    "    b = tf.Variable(tf.zeros(hidden_layers[i+1]))\n",
    "    Ws.append((W,b))\n",
    "W = tf.Variable(tf.random_normal([hidden_layers[-1], classes]))\n",
    "b = tf.Variable(tf.zeros([classes]))\n",
    "Ws.append((W,b))\n",
    "\n",
    "if use_lstm:\n",
    "    initial_state = tf.placeholder(tf.float32, [None, hidden_layers[0]*2], name='state')\n",
    "    zero_state = np.zeros([hidden_layers[0]*2], np.float32)\n",
    "    rnn_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layers[0], \n",
    "                                            forget_bias=0.0, state_is_tuple=False)\n",
    "else:\n",
    "    initial_state = tf.placeholder(tf.float32, [None, hidden_layers[0]], name='state')\n",
    "    zero_state = np.zeros([hidden_layers[0]], np.float32)\n",
    "    rnn_cell = tf.contrib.rnn.BasicRNNCell(hidden_layers[0])\n",
    "    \n",
    "#TODO dropout and multirnn\n",
    "if input_steps is None:\n",
    "    outputs, state = tf.nn.dynamic_rnn(rnn_cell, x, sequence_length=seqlen,\n",
    "                                   initial_state=initial_state, dtype=tf.float32)\n",
    "else:\n",
    "    outputs, state = tf.nn.dynamic_rnn(rnn_cell, x,\n",
    "                                   initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "if use_labels_data:\n",
    "    #before: [batchsize, timesteps, classes]\n",
    "    #then: list of size batchsize of [timesteps, classes] \n",
    "    #after concat: [batchsize*timesteps, classes]\n",
    "    y = tf.placeholder(tf.float32, [None, input_steps, classes], name='y')\n",
    "    ys = tf.unstack(y, axis=1)\n",
    "    y2 = tf.concat(ys, axis=0)\n",
    "    outputs2 = tf.unstack(outputs, axis=1)\n",
    "    \n",
    "    #calc outputs\n",
    "    rnn_outs = []\n",
    "    for out in outputs2:\n",
    "        preactivations = []\n",
    "        activations = []\n",
    "        act = out\n",
    "        for W,b in Ws:\n",
    "            preact = tf.matmul(act, W) + b\n",
    "            preactivations.append(preact)\n",
    "            act = tf.nn.relu(preact)\n",
    "            #act = tf.sigmoid(preact)\n",
    "            activations.append(act)\n",
    "        rnn_outs.append(preact)\n",
    "    #prediction for the last timestep\n",
    "    rnn_final = tf.nn.softmax(preact)\n",
    "    activations[-1] = rnn_final\n",
    "    rnn_pred = tf.argmax(rnn_final, 1, name='RNN_pred')\n",
    "    \n",
    "    #predictions for all timestep (needed for loss and accuracy)\n",
    "    preact = tf.concat(rnn_outs, axis=0)\n",
    "    rnn = tf.nn.softmax(preact)\n",
    "elif use_ctc:\n",
    "    y = tf.sparse_placeholder(tf.int32, name='y')\n",
    "    outputs = tf.reshape(outputs, [-1, hidden_layers[0]])\n",
    "\n",
    "    preactivations = []\n",
    "    activations = []\n",
    "    act = outputs\n",
    "    for W,b in Ws:\n",
    "        preact = tf.matmul(act, W) + b\n",
    "        preactivations.append(preact)\n",
    "        act = tf.nn.relu(preact)\n",
    "        #act = tf.sigmoid(preact)\n",
    "        activations.append(act)\n",
    "    rnn = tf.nn.softmax(preact)\n",
    "    activations[-1] = rnn\n",
    "    rnn_pred = tf.argmax(rnn, 1, name='RNN_pred')\n",
    "    preact = tf.reshape(preact, [tf.shape(x)[0],-1,classes])\n",
    "    \n",
    "else:\n",
    "    y = tf.placeholder(tf.float32, [None, classes], name='y')\n",
    "    y2 = y\n",
    "    outputs = tf.transpose(outputs, [1,0,2])\n",
    "\n",
    "    preactivations = []\n",
    "    activations = []\n",
    "    act = outputs[-1]\n",
    "    for W,b in Ws:\n",
    "        preact = tf.matmul(act, W) + b\n",
    "        preactivations.append(preact)\n",
    "        act = tf.nn.relu(preact)\n",
    "        #act = tf.sigmoid(preact)\n",
    "        activations.append(act)\n",
    "    rnn = tf.nn.softmax(preact)\n",
    "    activations[-1] = rnn\n",
    "    rnn_pred = tf.argmax(rnn, 1, name='RNN_pred')\n",
    "\n",
    "#important: logits=op before calling softmax!!\n",
    "if use_ctc:\n",
    "    ctc_loss = tf.nn.ctc_loss(inputs=preact, labels=y, sequence_length=seqlen, time_major=False)\n",
    "    loss = tf.reduce_mean(ctc_loss)\n",
    "    decoded, log_prob = tf.nn.ctc_greedy_decoder(preact, seqlen)\n",
    "    accuracy = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32),\n",
    "                                          y))\n",
    "else:\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y2, logits=preact))\n",
    "    correct_prediction = tf.equal(tf.argmax(y2,1), tf.argmax(rnn,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#summaries\n",
    "def var_summary(var, name):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar(name + '_mean', mean)\n",
    "    stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar(name + '_stddev', stddev)\n",
    "    tf.summary.scalar(name + '_max', tf.reduce_max(var))\n",
    "    tf.summary.scalar(name + '_min', tf.reduce_min(var))\n",
    "    tf.summary.histogram(name + '_histogram', var)\n",
    "i = 0\n",
    "for W,b in Ws: \n",
    "    var_summary(W, 'weights_%s' % i)\n",
    "    var_summary(b, 'bias_%s' % i)\n",
    "    i += 1\n",
    "for i in range(len(activations)):\n",
    "    tf.summary.histogram('preactivations_%s' % i, preactivations[i])\n",
    "    tf.summary.histogram('activations_%s' % i, activations[i])\n",
    "tf.summary.scalar('ctc_loss' if use_ctc else 'cross_entropy', loss)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('tmp/rnn/summary-%s' % model_num, sess.graph)\n",
    "\n",
    "#init or load from checkpoint\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "#save model meta graph\n",
    "meta_graph_def = tf.train.export_meta_graph(filename='tmp/rnn/model/rnn-%s.meta' % model_num)\n",
    "\n",
    "if load_rnn:\n",
    "    saver.restore(sess, 'tmp/rnn/model/rnn-%s/rnn-%s-%s' % (model_num,model_num,step_counter))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#not needed anymore\n",
    "def convert_to_onehot(labels):\n",
    "    new_labels = []\n",
    "    for y in labels:\n",
    "        label = [0]*classes\n",
    "        label[y] = 1\n",
    "        new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n]*len(seq), range(len(seq))))\n",
    "        values.extend(seq)\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1]+1], dtype=np.int64)\n",
    "\n",
    "    return indices, values, shape\n",
    "\n",
    "def test_all():\n",
    "    xtest,ytest,seq = samples.get_all(use_labels_data=use_labels_data)\n",
    "    #ytest = convert_to_onehot(ytest)\n",
    "    if use_ctc: \n",
    "        ytest2 = []\n",
    "        for yk in ytest:\n",
    "            ytest2.append([yk])\n",
    "        ytest = sparse_tuple_from(ytest2)\n",
    "    acc = sess.run(accuracy, feed_dict={x: xtest, y: ytest, seqlen: seq,\n",
    "                                        initial_state: [zero_state]*len(xtest)})\n",
    "    print(\"Accuracy: %s\" % acc)\n",
    "    \n",
    "def test():\n",
    "    xtest,ytest,seq = samples.get_test(use_labels_data=use_labels_data)\n",
    "    #ytest = convert_to_onehot(ytest)\n",
    "    acc = sess.run(accuracy, feed_dict={x: xtest, y: ytest, seqlen: seq,\n",
    "                                        initial_state: [zero_state]*len(xtest)})\n",
    "    print(\"Accuracy: %s\" % acc)\n",
    "    \n",
    "def per_label_acc(test=True):\n",
    "    if test:\n",
    "        xtest,ytest,seq = samples.get_test(use_labels_data=use_labels_data)\n",
    "    else:\n",
    "        xtest,ytest,seq = samples.get_all(use_labels_data=use_labels_data)\n",
    "    accs = [0] * classes\n",
    "    counts = [0] * classes\n",
    "\n",
    "    for i in range(len(ytest)):\n",
    "        acc = sess.run(accuracy, feed_dict={x: [xtest[i]], y: [ytest[i]], seqlen: [seq[i]],\n",
    "                                        initial_state: [zero_state]})\n",
    "        c = 0\n",
    "        if use_labels_data:\n",
    "            for y_oh in ytest[i]:\n",
    "                for k in range(len(y_oh)):\n",
    "                    if y_oh[k] > 0.5:\n",
    "                        c = k\n",
    "                        break\n",
    "                accs[c] += acc\n",
    "                counts[c] += 1\n",
    "        else:\n",
    "            for k in range(len(ytest[i])):\n",
    "                if ytest[i][k] > 0.5:\n",
    "                    c = k\n",
    "                    break\n",
    "            accs[c] += acc\n",
    "            counts[c] += 1\n",
    "\n",
    "    for i in range(classes):\n",
    "        acc = accs[i]/counts[i]\n",
    "        print(counts[i])\n",
    "        print('%s: %s' % (labels[i],acc))\n",
    "\n",
    "def train(steps, batch_size):\n",
    "    global step_counter\n",
    "    progress = 5\n",
    "    for i in range(steps):\n",
    "        xtrain,ytrain,seq = samples.get_batch(batch_size,\n",
    "                                              use_labels_data=use_labels_data)\n",
    "        #ytrain = convert_to_onehot(ytrain)\n",
    "        feed = {x: xtrain, y: ytrain, \n",
    "                seqlen: seq, initial_state: [zero_state]*len(xtrain)}\n",
    "        summary,_ = sess.run([merged, optimizer], feed_dict=feed)\n",
    "        train_writer.add_summary(summary, i+step_counter)\n",
    "        if i>=progress/100.0*steps-1:\n",
    "            print(\"Progress: %s%%\" % progress)\n",
    "            progress += 5\n",
    "            test()\n",
    "            saver.save(sess, 'tmp/rnn/model/rnn-%s/rnn-%s' % (model_num,model_num), global_step=step_counter)\n",
    "    step_counter += steps\n",
    "    saver.save(sess, 'tmp/rnn/model/rnn-%s/rnn-%s' % (model_num,model_num), global_step=step_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if input_steps is None:\n",
    "    def feed_data(data):\n",
    "        return sess.run(rnn_pred, feed_dict={x:[data], seqlen:[len(data)], initial_state:[zero_state]})\n",
    "\n",
    "    old_state = zero_state\n",
    "\n",
    "    def reset():\n",
    "        global old_state\n",
    "        old_state = zero_state\n",
    "\n",
    "    def feed_seq(s):\n",
    "        global old_state\n",
    "        res,new_state = sess.run([rnn_pred,state], feed_dict={x:[s], seqlen:[len(s)], initial_state:[old_state]})\n",
    "        old_state = new_state[-1]\n",
    "        return res[-1]\n",
    "else:\n",
    "    def feed_data(data):\n",
    "        return sess.run(rnn_pred, feed_dict={x:[data], seqlen:[len(data)], initial_state:[zero_state]})\n",
    "    \n",
    "    old_state = zero_state\n",
    "    xs = []\n",
    "    \n",
    "    def reset():\n",
    "        global old_state\n",
    "        old_state = zero_state\n",
    "    \n",
    "    def feed_seq(s):\n",
    "        global old_state\n",
    "        global xs\n",
    "        xs.extend(s)\n",
    "        if len(xs) > input_steps:\n",
    "            del xs[:len(xs)-input_steps]\n",
    "            assert len(xs) == input_steps\n",
    "            \n",
    "        if len(xs) == input_steps:\n",
    "            res,new_state = sess.run([rnn_pred,state], feed_dict={x:[xs], \n",
    "                                seqlen:[len(xs)], initial_state:[old_state]})\n",
    "            old_state = new_state[-1]\n",
    "            return res\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def feed_one(s):\n",
    "        global old_state_second\n",
    "        global old_state_last\n",
    "        res,new_state = sess.run([rnn_pred,state], feed_dict={x:[s], \n",
    "                            seqlen:[len(s)], initial_state:[old_state]})\n",
    "        old_state = new_state[-1]\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "if True:\n",
    "    test_all()\n",
    "    train(1000,100)\n",
    "    test_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    per_label_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tests whether the network only outputs zero (to make sure accuracy isnt skewed)\n",
    "if False:\n",
    "    index = 3\n",
    "    for i in range(len(labels)):\n",
    "        print('%s: %s' % (i,labels[i]))\n",
    "    print('should be %s - %s' % (index, labels[index]))\n",
    "    sample = samples.data_old[labels[index]][40]\n",
    "    count = 0\n",
    "    for i in range(10,len(sample)):\n",
    "        res = feed_data(sample[:i])\n",
    "        if res == index: count += 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#proof of memory\n",
    "if False:\n",
    "    a = [0,0,0,0,0,0]\n",
    "    b = [0,2,0,5,-100,0]\n",
    "    \n",
    "    reset()\n",
    "    data1 = [b,a,a,a,a]\n",
    "    print(feed_data(data1))\n",
    "    reset()\n",
    "    for i in data1:\n",
    "        print(feed_seq([i]))\n",
    "    print(feed_seq([a]))\n",
    "    reset()\n",
    "    for i in data1:\n",
    "        print(feed_seq([i]))\n",
    "    print(feed_seq([b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#live demo\n",
    "%env ROS_HOSTNAME=zinunb\n",
    "%env ROS_IP=$(hostname -I)\n",
    "%env ROS_MASTER_URI=http://pi:11311\n",
    "\n",
    "if False:\n",
    "    import imu_listener as il\n",
    "    reset()\n",
    "    il.imu_listener(feed_seq,labels,1)\n",
    "    print('Exited')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
