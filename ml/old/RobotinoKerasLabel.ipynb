{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 102_10 (type 102, num 10)\n",
      "1 classes, input_dim 6\n"
     ]
    }
   ],
   "source": [
    "#model parameters\n",
    "import numpy as np\n",
    "import samples as ls\n",
    "import math\n",
    "import model_types\n",
    "\n",
    "#sgd 1_4, 2_4, 0_5, 10/11/12_34\n",
    "\n",
    "#adam: 1/2_7, 0_6\n",
    "#adam trained: 0_6, 2_7\n",
    "\n",
    "#used networks:\n",
    "# 0_6\n",
    "# 1/2_?\n",
    "# 10/11_34\n",
    "# 12_36\n",
    "# 11_? \n",
    "# 11_?\n",
    "\n",
    "load_test = False\n",
    "predict_mode = False\n",
    "\n",
    "load_rnn = False if not predict_mode else False\n",
    "load_only_weights = False if not load_rnn else False\n",
    "model_num = 10\n",
    "model_type = 102\n",
    "\n",
    "train_network = True\n",
    "\n",
    "#default model parameter\n",
    "all_classes = False\n",
    "accel,gyro,compass = 'xyz','xyz',''\n",
    "overlap_step = 1\n",
    "use_labels_data = True\n",
    "use_lstm = True\n",
    "use_sgd = False\n",
    "use_rnn = True\n",
    "use_labels_data = True\n",
    "input_steps = None\n",
    "unroll_steps = None\n",
    "activation = 'relu'\n",
    "load_samples = True\n",
    "dropout_prob = 0\n",
    "normalize = False\n",
    "binary = False\n",
    "\n",
    "model_id = '%s_%s' % (model_type, model_num)\n",
    "model_path = 'tmp/keraslabel/nn-%s.hdf5' % model_id\n",
    "weights_path = 'tmp/keraslabel/nn-%s_weights.hdf5' % model_id\n",
    "labels,sample_ids,filename = model_types.get_labels(model_type, load_test)\n",
    "\n",
    "if model_num == 0:\n",
    "    hidden_layers = [50]\n",
    "    unroll_steps = 100\n",
    "    overlap_steps = 50\n",
    "    all_classes = True\n",
    "    load_samples = False\n",
    "elif model_num == 1:\n",
    "    hidden_layers = [6]\n",
    "elif model_num == 2:\n",
    "    unroll_steps = 100\n",
    "    overlap_steps = 25\n",
    "    hidden_layers = [50]\n",
    "elif model_num == 4:\n",
    "    unroll_steps = 10\n",
    "    overlap_steps = 1\n",
    "    hidden_layers = [5]\n",
    "elif model_num == 10:\n",
    "    hidden_layers = [6]\n",
    "    binary = True\n",
    "    \n",
    "    \n",
    "input_dim = len(accel)+len(gyro)+len(compass)\n",
    "\n",
    "assert not binary or not all_classes\n",
    "\n",
    "classes = len(labels) if all_classes else 1 if binary else 2\n",
    "\n",
    "#percentage used for test data\n",
    "test_rate = 0.1\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "print('model: %s (type %s, num %s)' % (model_id,model_type,model_num))\n",
    "print('%s classes, input_dim %s' % (classes, input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "straight: 36 / 4\n",
      "curve_m_l: 54 / 6\n",
      "curve_m_r: 54 / 6\n",
      "wall: 90 / 10\n",
      "foot: 72 / 8\n",
      "object_l: 72 / 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object_r: 72 / 8\n",
      "432\n"
     ]
    }
   ],
   "source": [
    "#load samples\n",
    "if load_samples:\n",
    "    samples = ls.load(filename)\n",
    "else:\n",
    "    samples = ls.Samples(labels)\n",
    "    \n",
    "    for label in labels:\n",
    "        samples.load_samples(label = label, sample_ids = sample_ids)\n",
    "\n",
    "    samples.convert_to_input(size = None, accel=accel, gyro=gyro, compass=compass)\n",
    "\n",
    "samples.unroll(unroll_steps, overlap_step)\n",
    "samples.convert_to_onehot()\n",
    "samples.split_test(test_rate, random=False)\n",
    "if not use_rnn: samples.flatten()\n",
    "    \n",
    "    \n",
    "sumlen = 0\n",
    "for key in labels:\n",
    "    print('%s: %s / %s' % (key, len(samples.data[key]), len(samples.test[key])))\n",
    "    if key != 'straight': sumlen += len(samples.data[key])\n",
    " \n",
    "#extend straight\n",
    "data = samples.data['straight'][:]\n",
    "data2 = samples.labels_data['straight'][:]\n",
    "while len(samples.data['straight']) < sumlen:\n",
    "    samples.data['straight'].extend(data)\n",
    "    samples.labels_data['straight'].extend(data2)\n",
    "print(len(samples.data['straight']))\n",
    "\n",
    "if not all_classes:\n",
    "    for key in labels:\n",
    "        for sample in range(len(samples.labels_data[key])):\n",
    "            for s in range(len(samples.labels_data[key][sample])):\n",
    "                for step in range(len(samples.labels_data[key][sample][s])):\n",
    "                    if key == 'straight':\n",
    "                        samples.labels_data[key][sample][s][step] = [1.0,0.0] if not binary else [0]\n",
    "                    else:\n",
    "                        samples.labels_data[key][sample][s][step] = [0.0,1.0] if not binary else [1]\n",
    "        for sample in range(len(samples.labels_test[key])):\n",
    "            for s in range(len(samples.labels_test[key][sample])):\n",
    "                for step in range(len(samples.labels_test[key][sample][s])):\n",
    "                    if key == 'straight':\n",
    "                        samples.labels_test[key][sample][s][step] = [1.0,0.0] if not binary else [0]\n",
    "                    else:\n",
    "                        samples.labels_test[key][sample][s][step] = [0.0,1.0] if not binary else [1]\n",
    "                        \n",
    "                        \n",
    "#prepare training data\n",
    "xtrain,ytrain,seqlen = samples.get_all(padding=False, use_labels_data=use_labels_data)\n",
    "xtest,ytest,seqlentest = samples.get_test(padding=False, use_labels_data=use_labels_data)\n",
    "\n",
    "\n",
    "#normalize data\n",
    "if normalize:\n",
    "    \n",
    "    mean = [0]*input_dim\n",
    "    maxx = [-100000]*input_dim\n",
    "    minx = [1000000]*input_dim\n",
    "    for dataset in [xtrain,xtest]:\n",
    "        for x in dataset:\n",
    "            for d in input_dim:\n",
    "                mean += x[d]\n",
    "                if maxx[d] < x[d]: maxx[d] = x[d]\n",
    "                if minx[d] > x[d]: minx[d] = x[d]\n",
    "\n",
    "    for d in input_dim:\n",
    "        mean[d] = mean[d]/(len(xtrain)+len(xtest))\n",
    "        maxx[d] -= mean[d]\n",
    "        minx[d] -= mean[d]\n",
    "        \n",
    "    \n",
    "    def norm(x):\n",
    "        for d in input_dim:\n",
    "            x[d] -= mean[d]\n",
    "            x[d] = (x[d] - minx[d]) / (maxx[d] - minx[d])\n",
    "            x[d] = x[d]*2 - 1\n",
    "        \n",
    "    for dataset in [xtrain,xtest]:\n",
    "        for x in dataset:\n",
    "            norm(x)\n",
    "\n",
    "    print(mean)\n",
    "    print(maxx)\n",
    "    print(minx)\n",
    "\n",
    "\n",
    "\n",
    "if unroll_steps is None:\n",
    "    from keras.preprocessing import sequence\n",
    "    xtrain = sequence.pad_sequences(xtrain)\n",
    "    xtest = sequence.pad_sequences(xtest)\n",
    "    #not efficient but whatevs\n",
    "    if len(xtrain[0]) > len(xtest[0]):\n",
    "        input_steps = len(xtrain[0])\n",
    "        xtest = sequence.pad_sequences(xtest, maxlen=input_steps)\n",
    "    elif len(xtrain[0]) < len(xtest[0]):\n",
    "        input_steps = len(xtest[0])\n",
    "        xtrain = sequence.pad_sequences(xtrain, maxlen=input_steps)\n",
    "    else:\n",
    "        input_steps = len(xtrain[0])\n",
    "    \n",
    "    ytrain = sequence.pad_sequences(ytrain, maxlen=input_steps)\n",
    "    ytest = sequence.pad_sequences(ytest, maxlen=input_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.core import Masking\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "if load_rnn and not load_only_weights:\n",
    "    model = load_model(model_path)\n",
    "    model.load_weights(weights_path)\n",
    "else:\n",
    "    model = Sequential()\n",
    "    last_act = 'sigmoid' if binary else 'softmax'\n",
    "    if unroll_steps is None:\n",
    "        model.add(Masking(input_shape=(input_steps, input_dim)))\n",
    "        \n",
    "        if use_lstm:\n",
    "            model.add(LSTM(hidden_layers.pop(0), dropout=dropout_prob, recurrent_dropout=dropout_prob, \n",
    "                       return_sequences=use_labels_data))\n",
    "        else:\n",
    "            model.add(SimpleRNN(hidden_layers.pop(0), dropout=dropout_prob, recurrent_dropout=dropout_prob, \n",
    "                       return_sequences=use_labels_data))\n",
    "                     \n",
    "        if use_labels_data:\n",
    "            for k in hidden_layers:\n",
    "                model.add(TimeDistributed(Dense(k, activation=activation)))\n",
    "            model.add(TimeDistributed(Dense(classes, activation=last_act)))\n",
    "        else:\n",
    "            for k in hidden_layers:\n",
    "                model.add(Dense(k, activation=activation))\n",
    "            model.add(Dense(classes, activation=last_act))\n",
    "        \n",
    "    elif use_rnn:\n",
    "        if use_lstm:\n",
    "            if predict_mode:\n",
    "                model.add(LSTM(hidden_layers.pop(0), dropout=dropout_prob, recurrent_dropout=dropout_prob, \n",
    "                       batch_input_shape=(1, input_steps, input_dim), return_sequences=use_labels_data, stateful=True))\n",
    "            else:\n",
    "                model.add(LSTM(hidden_layers.pop(0), dropout=dropout_prob, recurrent_dropout=dropout_prob, \n",
    "                       input_shape=(input_steps, input_dim), return_sequences=use_labels_data))\n",
    "        else:\n",
    "            if predict_mode:\n",
    "                model.add(SimpleRNN(hidden_layers.pop(0), dropout=dropout_prob, recurrent_dropout=dropout_prob, \n",
    "                       batch_input_shape=(1, input_steps, input_dim), return_sequences=use_labels_data, stateful=True))\n",
    "            else:\n",
    "                model.add(SimpleRNN(hidden_layers.pop(0), dropout=dropout_prob, recurrent_dropout=dropout_prob, \n",
    "                       input_shape=(input_steps, input_dim), return_sequences=use_labels_data))\n",
    "                     \n",
    "        if use_labels_data:\n",
    "            for k in hidden_layers:\n",
    "                model.add(TimeDistributed(Dense(k, activation=activation)))\n",
    "            model.add(TimeDistributed(Dense(classes, activation=last_act)))\n",
    "        else:\n",
    "            for k in hidden_layers:\n",
    "                model.add(Dense(k, activation=activation))\n",
    "            model.add(Dense(classes, activation=last_act))\n",
    "    else:\n",
    "        if not hidden_layers:\n",
    "            model.add(Dense(classes, activation=last_act, input_shape=(input_steps*input_dim,)))\n",
    "        else:\n",
    "            model.add(Dense(hidden_layers.pop(0), activation=activation, input_shape=(input_steps*input_dim,)))\n",
    "            for k in hidden_layers:\n",
    "                model.add(Dense(k, activation=activation))\n",
    "            model.add(Dense(classes, activation=last_act))\n",
    "   \n",
    "        \n",
    "    \n",
    "\n",
    "    if use_sgd: opt = optimizers.SGD(lr=learning_rate)\n",
    "    else: opt = 'adam'\n",
    "        \n",
    "    if binary:\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    if predict_mode or load_only_weights:\n",
    "        model.load_weights(weights_path)\n",
    "    else:\n",
    "        model.save(model_path)\n",
    "        model.save_weights(weights_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if not predict_mode:\n",
    "    def train(epochs, batch_size):\n",
    "        mcp = ModelCheckpoint(weights_path, monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "        model.fit(xtrain, ytrain,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(xtest, ytest),\n",
    "                  callbacks=[mcp])\n",
    "        score, acc = model.evaluate(xtest, ytest, batch_size=len(xtest))\n",
    "        print('Test score: %s' % score)\n",
    "        print('Test accuracy: %s' % acc)\n",
    "        #model.save(model_path)\n",
    "        #model.save_weights(weights_path)\n",
    "        \n",
    "    \n",
    "    def test_total(batch_size=None):\n",
    "        if batch_size == None:\n",
    "            acc1 = model.evaluate(xtrain, ytrain, batch_size=len(xtrain), verbose=0)[1]\n",
    "            acc2 = model.evaluate(xtest, ytest, batch_size=len(xtest), verbose=0)[1]\n",
    "        else:\n",
    "            acc1 = 0\n",
    "            num = len(xtrain)\n",
    "            for i in range(0,num,batch_size):\n",
    "                size = min(batch_size, len(xtrain)-i)\n",
    "                xtrain2 = xtrain[i:i+size]\n",
    "                ytrain2 = ytrain[i:i+size]\n",
    "                acc1 += (float(size)/num) * model.evaluate(xtrain2, ytrain2, batch_size=size, verbose=0)[1]\n",
    "                \n",
    "            acc2 = 0\n",
    "            num = len(xtest)\n",
    "            for i in range(0,num,batch_size):\n",
    "                size = min(batch_size, len(xtest)-i)\n",
    "                xtrain2 = xtest[i:i+size]\n",
    "                ytrain2 = ytest[i:i+size]\n",
    "                acc2 += (float(size)/num) * model.evaluate(xtrain2, ytrain2, batch_size=size, verbose=0)[1]\n",
    "        acc = acc1*(1.0-test_rate) + acc2*test_rate\n",
    "        print('Train accuracy: %s' % acc1)\n",
    "        print('Test accuracy: %s' % acc2)\n",
    "        print('Total accuracy: %s' % acc)\n",
    "        \n",
    "    def test():\n",
    "        score, acc = model.evaluate(xtest, ytest, batch_size=len(xtest))\n",
    "        print('Test score: %s' % score)\n",
    "        print('Test accuracy: %s' % acc)\n",
    "        \n",
    "    \n",
    "    def per_label_acc(test=True):\n",
    "        if test:\n",
    "            x,y = xtest,ytest\n",
    "        else:\n",
    "            x,y = xtrain,ytrain\n",
    "        accs = [0] * classes\n",
    "        counts = [0] * classes\n",
    "        \n",
    "        print_step = len(y)/10\n",
    "        for i in range(len(y)):\n",
    "            acc = model.evaluate(np.array([x[i]]), np.array([y[i]]), batch_size=1, verbose=0)[1]\n",
    "            c = 0\n",
    "            if use_labels_data:\n",
    "                for y_oh in y[i]:\n",
    "                    c = np.argmax(y_oh)\n",
    "                    accs[c] += acc\n",
    "                    counts[c] += 1\n",
    "            else:\n",
    "                c = np.argmax(y[i])\n",
    "                accs[c] += acc\n",
    "                counts[c] += 1\n",
    "            if i%print_step==0:\n",
    "                print('Progress: %s' % (float(i)/len(y)))\n",
    "        \n",
    "        acc_total = 0\n",
    "        for i in range(classes):\n",
    "            acc = accs[i]/counts[i]\n",
    "            print('%sx %s: %s' % (counts[i],labels[i],acc))\n",
    "            acc_total += acc*1.0/classes\n",
    "        print('Total test acc: %s' % acc_total)\n",
    "    \n",
    "def feed_seq(s, as_prob=False):\n",
    "    res = model.predict(np.array([s]),batch_size=1)[0]\n",
    "    resarg = []\n",
    "    for r in res:\n",
    "        if as_prob: resarg.append(r)\n",
    "        else: resarg.append(np.argmax(r))\n",
    "    return resarg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if predict_mode:\n",
    "    if False:\n",
    "        import time\n",
    "        start = time.time()\n",
    "        for i in range(10000):\n",
    "            z = [0,0,0,0,0,0]\n",
    "            v = [z]*1\n",
    "            feed_seq(v)\n",
    "        dt = time.time() - start\n",
    "        print('%s hz' % (1/(dt/10000)))\n",
    "\n",
    "    if True:\n",
    "        for key in samples.test:\n",
    "            model.reset_states()\n",
    "            xdata = samples.test[key][0][0]\n",
    "            ydata = samples.labels_test[key][0][0]\n",
    "\n",
    "            res = feed_seq(xdata)\n",
    "\n",
    "            res_predicted = res\n",
    "            res_expected = []\n",
    "            acc = 0\n",
    "            for i in range(len(res)):\n",
    "                res_expected.append(np.argmax(ydata[i]))\n",
    "                if res_predicted[len(res_expected)-1] == res_expected[-1]:\n",
    "                    acc += 1.0\n",
    "            print('%s: %s' % (key, acc/len(res)))\n",
    "            print(res_predicted)\n",
    "            print(res_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 846 samples, validate on 50 samples\n",
      "Epoch 1/2000\n",
      "846/846 [==============================] - 8s - loss: 0.6862 - acc: 0.5598 - val_loss: 0.7005 - val_acc: 0.2732\n",
      "Epoch 2/2000\n",
      "846/846 [==============================] - 8s - loss: 0.6830 - acc: 0.5799 - val_loss: 0.6878 - val_acc: 0.3385\n",
      "Epoch 3/2000\n",
      "846/846 [==============================] - 9s - loss: 0.6796 - acc: 0.6191 - val_loss: 0.6730 - val_acc: 0.5001\n",
      "Epoch 4/2000\n",
      "846/846 [==============================] - 9s - loss: 0.6767 - acc: 0.5149 - val_loss: 0.6606 - val_acc: 0.7275\n",
      "Epoch 5/2000\n",
      "846/846 [==============================] - 9s - loss: 0.6723 - acc: 0.6480 - val_loss: 0.6696 - val_acc: 0.4835\n",
      "Epoch 6/2000\n",
      "846/846 [==============================] - 10s - loss: 0.6663 - acc: 0.6570 - val_loss: 0.6628 - val_acc: 0.5122\n",
      "Epoch 7/2000\n",
      "846/846 [==============================] - 9s - loss: 0.6577 - acc: 0.6912 - val_loss: 0.6526 - val_acc: 0.5489\n",
      "Epoch 8/2000\n",
      "846/846 [==============================] - 9s - loss: 0.6429 - acc: 0.6342 - val_loss: 0.6867 - val_acc: 0.4208\n",
      "Epoch 9/2000\n",
      "846/846 [==============================] - 9s - loss: 0.5967 - acc: 0.7302 - val_loss: 0.6772 - val_acc: 0.5276\n",
      "Epoch 10/2000\n",
      "846/846 [==============================] - 9s - loss: 0.5858 - acc: 0.7023 - val_loss: 0.7914 - val_acc: 0.3636\n",
      "Epoch 11/2000\n",
      "846/846 [==============================] - 9s - loss: 0.5578 - acc: 0.7388 - val_loss: 0.5697 - val_acc: 0.6834\n",
      "Epoch 12/2000\n",
      "846/846 [==============================] - 9s - loss: 0.5384 - acc: 0.7682 - val_loss: 0.6188 - val_acc: 0.5639\n",
      "Epoch 13/2000\n",
      "846/846 [==============================] - 9s - loss: 0.5220 - acc: 0.7731 - val_loss: 0.4632 - val_acc: 0.7650\n",
      "Epoch 14/2000\n",
      "846/846 [==============================] - 9s - loss: 0.5081 - acc: 0.7785 - val_loss: 0.5708 - val_acc: 0.5703\n",
      "Epoch 15/2000\n",
      "846/846 [==============================] - 9s - loss: 0.5041 - acc: 0.7858 - val_loss: 0.4535 - val_acc: 0.6934\n",
      "Epoch 16/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4858 - acc: 0.7907 - val_loss: 0.4886 - val_acc: 0.6550\n",
      "Epoch 17/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4786 - acc: 0.7950 - val_loss: 0.4614 - val_acc: 0.6751\n",
      "Epoch 18/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4724 - acc: 0.7978 - val_loss: 0.4168 - val_acc: 0.7100\n",
      "Epoch 19/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4603 - acc: 0.7992 - val_loss: 0.4762 - val_acc: 0.6529\n",
      "Epoch 20/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4459 - acc: 0.8041 - val_loss: 0.4308 - val_acc: 0.6959\n",
      "Epoch 21/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4437 - acc: 0.8028 - val_loss: 0.4310 - val_acc: 0.6954\n",
      "Epoch 22/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4577 - acc: 0.7815 - val_loss: 0.4619 - val_acc: 0.6527\n",
      "Epoch 23/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4372 - acc: 0.8026 - val_loss: 0.4498 - val_acc: 0.6555\n",
      "Epoch 24/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4238 - acc: 0.8080 - val_loss: 0.4017 - val_acc: 0.7024\n",
      "Epoch 25/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4237 - acc: 0.8059 - val_loss: 0.4947 - val_acc: 0.6328\n",
      "Epoch 26/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4144 - acc: 0.8042 - val_loss: 0.4971 - val_acc: 0.6369\n",
      "Epoch 27/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4059 - acc: 0.8142 - val_loss: 0.3688 - val_acc: 0.7186\n",
      "Epoch 28/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4177 - acc: 0.8046 - val_loss: 0.4287 - val_acc: 0.6791\n",
      "Epoch 29/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4047 - acc: 0.8093 - val_loss: 0.4727 - val_acc: 0.6462\n",
      "Epoch 30/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4023 - acc: 0.8099 - val_loss: 0.4829 - val_acc: 0.6409\n",
      "Epoch 31/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4032 - acc: 0.8104 - val_loss: 0.4882 - val_acc: 0.6404\n",
      "Epoch 32/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3919 - acc: 0.8120 - val_loss: 0.4591 - val_acc: 0.6507\n",
      "Epoch 33/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3880 - acc: 0.8160 - val_loss: 0.4747 - val_acc: 0.6469\n",
      "Epoch 34/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3851 - acc: 0.8115 - val_loss: 0.4502 - val_acc: 0.6580\n",
      "Epoch 35/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3921 - acc: 0.8100 - val_loss: 0.5579 - val_acc: 0.6168\n",
      "Epoch 36/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4007 - acc: 0.8061 - val_loss: 0.4511 - val_acc: 0.6583\n",
      "Epoch 37/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3847 - acc: 0.8185 - val_loss: 0.4566 - val_acc: 0.6568\n",
      "Epoch 38/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3809 - acc: 0.8108 - val_loss: 0.4781 - val_acc: 0.6492\n",
      "Epoch 39/2000\n",
      "846/846 [==============================] - 9s - loss: 0.4099 - acc: 0.7969 - val_loss: 0.6422 - val_acc: 0.5635\n",
      "Epoch 40/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3986 - acc: 0.8083 - val_loss: 0.4650 - val_acc: 0.6497\n",
      "Epoch 41/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3941 - acc: 0.8038 - val_loss: 0.4596 - val_acc: 0.6568\n",
      "Epoch 42/2000\n",
      "846/846 [==============================] - 10s - loss: 0.3811 - acc: 0.8147 - val_loss: 0.4374 - val_acc: 0.6613\n",
      "Epoch 43/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3733 - acc: 0.8186 - val_loss: 0.4620 - val_acc: 0.6575\n",
      "Epoch 44/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3735 - acc: 0.8167 - val_loss: 0.4815 - val_acc: 0.6500\n",
      "Epoch 45/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3738 - acc: 0.8157 - val_loss: 0.4596 - val_acc: 0.6598\n",
      "Epoch 46/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3701 - acc: 0.8155 - val_loss: 0.4248 - val_acc: 0.6683\n",
      "Epoch 47/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3766 - acc: 0.8154 - val_loss: 0.4572 - val_acc: 0.6618\n",
      "Epoch 48/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3677 - acc: 0.8186 - val_loss: 0.4143 - val_acc: 0.6746\n",
      "Epoch 49/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3679 - acc: 0.8194 - val_loss: 0.4944 - val_acc: 0.6472\n",
      "Epoch 50/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3787 - acc: 0.8108 - val_loss: 0.4767 - val_acc: 0.6545\n",
      "Epoch 51/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3734 - acc: 0.8138 - val_loss: 0.4489 - val_acc: 0.6641\n",
      "Epoch 52/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3667 - acc: 0.8195 - val_loss: 0.4709 - val_acc: 0.6593\n",
      "Epoch 53/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3683 - acc: 0.8171 - val_loss: 0.4948 - val_acc: 0.6513\n",
      "Epoch 54/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3698 - acc: 0.8143 - val_loss: 0.4224 - val_acc: 0.6726\n",
      "Epoch 55/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3698 - acc: 0.8152 - val_loss: 0.4225 - val_acc: 0.6736\n",
      "Epoch 56/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3642 - acc: 0.8179 - val_loss: 0.5052 - val_acc: 0.6455\n",
      "Epoch 57/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3709 - acc: 0.8123 - val_loss: 0.4935 - val_acc: 0.6505\n",
      "Epoch 58/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3686 - acc: 0.8144 - val_loss: 0.3854 - val_acc: 0.6887\n",
      "Epoch 59/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3699 - acc: 0.8124 - val_loss: 0.4165 - val_acc: 0.6764\n",
      "Epoch 60/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3660 - acc: 0.8174 - val_loss: 0.3780 - val_acc: 0.6937\n",
      "Epoch 61/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3641 - acc: 0.8187 - val_loss: 0.4509 - val_acc: 0.6668\n",
      "Epoch 62/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3668 - acc: 0.8162 - val_loss: 0.4722 - val_acc: 0.6583\n",
      "Epoch 63/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3571 - acc: 0.8233 - val_loss: 0.3996 - val_acc: 0.6824\n",
      "Epoch 64/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3596 - acc: 0.8183 - val_loss: 0.4494 - val_acc: 0.6681\n",
      "Epoch 65/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846/846 [==============================] - 9s - loss: 0.3622 - acc: 0.8155 - val_loss: 0.4441 - val_acc: 0.6691\n",
      "Epoch 66/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3593 - acc: 0.8202 - val_loss: 0.4780 - val_acc: 0.6626\n",
      "Epoch 67/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3577 - acc: 0.8195 - val_loss: 0.4710 - val_acc: 0.6598\n",
      "Epoch 68/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3675 - acc: 0.8152 - val_loss: 0.4981 - val_acc: 0.6495\n",
      "Epoch 69/2000\n",
      "846/846 [==============================] - 13s - loss: 0.3625 - acc: 0.8163 - val_loss: 0.4804 - val_acc: 0.6560\n",
      "Epoch 70/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3614 - acc: 0.8173 - val_loss: 0.4779 - val_acc: 0.6608\n",
      "Epoch 71/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3585 - acc: 0.8197 - val_loss: 0.4193 - val_acc: 0.6834\n",
      "Epoch 72/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3562 - acc: 0.8220 - val_loss: 0.4369 - val_acc: 0.6751\n",
      "Epoch 73/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3680 - acc: 0.8154 - val_loss: 0.4798 - val_acc: 0.6628\n",
      "Epoch 74/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3636 - acc: 0.8179 - val_loss: 0.4263 - val_acc: 0.6766\n",
      "Epoch 75/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3577 - acc: 0.8186 - val_loss: 0.4263 - val_acc: 0.6789\n",
      "Epoch 76/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3531 - acc: 0.8226 - val_loss: 0.5037 - val_acc: 0.6538\n",
      "Epoch 77/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3568 - acc: 0.8186 - val_loss: 0.4388 - val_acc: 0.6754\n",
      "Epoch 78/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3855 - acc: 0.8120 - val_loss: 0.4456 - val_acc: 0.6618\n",
      "Epoch 79/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3621 - acc: 0.8180 - val_loss: 0.4323 - val_acc: 0.6709\n",
      "Epoch 80/2000\n",
      "846/846 [==============================] - 9s - loss: 0.3639 - acc: 0.8139 - val_loss: 0.4119 - val_acc: 0.6812\n",
      "Epoch 81/2000\n",
      "846/846 [==============================] - 11s - loss: 0.3584 - acc: 0.8196 - val_loss: 0.4377 - val_acc: 0.6749\n",
      "Epoch 82/2000\n",
      "480/846 [================>.............] - ETA: 6s - loss: 0.3545 - acc: 0.8189"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0812d172dcb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#model.optimizer.lr.assign(0.00000000000001)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#per_label_acc()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d173df39549c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                   callbacks=[mcp])\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test score: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if train_network and not predict_mode:\n",
    "    #train(1000,32)\n",
    "    \n",
    "    #model.optimizer.lr.assign(0.00000000000001)\n",
    "    train(2000,40)\n",
    "    \n",
    "    #per_label_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if True and not predict_mode:\n",
    "    test_total(1000)\n",
    "    #test_total(len(xtest))\n",
    "    #test()\n",
    "    per_label_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if predict_mode and True:\n",
    "    %matplotlib notebook\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    DIM = input_dim\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    #ax = fig.add_axes([0.1,0.1,0.75,0.75])\n",
    "    plt.ion()\n",
    "    fig.show()\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    def draw_data_cutter(key, expected):\n",
    "        model.reset_states()\n",
    "        xdata = samples.test[key][0][0]\n",
    "        ydata = samples.labels_test[key][0][0]\n",
    "\n",
    "        res = feed_seq(xdata)\n",
    "        print(res)\n",
    "        \n",
    "        ax.clear()\n",
    "        ax.set_title('%s' % (key))\n",
    "        ax.set_xlabel('time')\n",
    "\n",
    "        data2 = np.array(xdata)\n",
    "        data = []\n",
    "        for i in range(DIM): data.append([])\n",
    "        for d in data2:\n",
    "            for i in range(DIM):\n",
    "                data[i].append(d[i])\n",
    "\n",
    "        for i in range(DIM):\n",
    "            if i<3 and DIM>3:\n",
    "                label = 'accel %s' % ('xyz'[i])\n",
    "            else:\n",
    "                label = 'gyro %s' % ('xyz'[i-3])\n",
    "            ax.plot(range(len(data[i])),data[i], label=label)\n",
    "        \n",
    "        key_expected = 0 if key=='straight' else 1\n",
    "        if all_classes:\n",
    "            key_expected = labels.index(key)\n",
    "        start_i = 0\n",
    "        current_type = 0\n",
    "        for i in range(len(res)):\n",
    "            if current_type != res[i]:\n",
    "                if current_type == key_expected:\n",
    "                    ax.axvspan(start_i, i, color='green', alpha=0.1)\n",
    "                elif current_type != key_expected and current_type != 0:\n",
    "                    ax.axvspan(start_i, i, color='red', alpha=0.1)\n",
    "                current_type = res[i]\n",
    "                start_i = i\n",
    "        if current_type == key_expected:\n",
    "            ax.axvspan(start_i, len(res), color='green', alpha=0.1)\n",
    "        elif current_type != key_expected and current_type != 0:\n",
    "            ax.axvspan(start_i, i, color='red', alpha=0.1)\n",
    "        \n",
    "        ax.legend(loc='lower left')\n",
    "        fig.canvas.draw()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if predict_mode and True:\n",
    "    draw_data_cutter('straight',False)\n",
    "    #for label in labels:\n",
    "    #    draw_data_cutter(label,False)\n",
    "    #    plt.savefig('data-%s-%s.png' % (label, 'predicted'),pad_inches=0,bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
