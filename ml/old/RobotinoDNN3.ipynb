{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model parameters\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import samples as ls\n",
    "import model_types\n",
    "import os\n",
    "\n",
    "step_counter = 0\n",
    "create_model = True\n",
    "load_dnn = True #ignored when create_model is set\n",
    "load_samples = False\n",
    "model_num = 2\n",
    "model_type = 11\n",
    "smooth_steps = 0\n",
    "\n",
    "model_id = '%s_%s' % (model_type, model_num)\n",
    "labels,sample_ids,filename = model_types.get_labels(model_type)\n",
    "model_path = 'tmp/dnn/model/dnn-%s' % model_id\n",
    "\n",
    "if model_num == 0:\n",
    "    accel,gyro,compass = 'xyz','xyz',''\n",
    "    input_dim = len(accel)+len(gyro)+len(compass)\n",
    "    hidden_layers = [50,10]\n",
    "elif model_num == 1:\n",
    "    accel,gyro,compass = 'xyz','xyz',''\n",
    "    input_dim = len(accel)+len(gyro)+len(compass)\n",
    "    hidden_layers = [50]\n",
    "elif model_num == 2:\n",
    "    accel,gyro,compass = 'xy','xyz',''\n",
    "    input_dim = len(accel)+len(gyro)+len(compass)\n",
    "    hidden_layers = [20,10]\n",
    "elif model_num == 3:\n",
    "    accel,gyro,compass = 'xy','xyz',''\n",
    "    input_dim = len(accel)+len(gyro)+len(compass)\n",
    "    hidden_layers = [20]\n",
    "elif model_num == 4:\n",
    "    accel,gyro,compass = 'xy','z',''\n",
    "    input_dim = len(accel)+len(gyro)+len(compass)\n",
    "    hidden_layers = [10]\n",
    "elif model_num == 5:\n",
    "    accel,gyro,compass = 'xy','xyz',''\n",
    "    input_dim = len(accel)+len(gyro)+len(compass)\n",
    "    hidden_layers = [10,10]\n",
    "elif model_num == 6:\n",
    "    accel,gyro,compass = 'xy','xyz',''\n",
    "    input_dim = len(accel)+len(gyro)+len(compass)\n",
    "    hidden_layers = [120,50]\n",
    "elif model_num == 7:\n",
    "    accel,gyro,compass = 'xy','xyz',''\n",
    "    input_dim = len(accel)+len(gyro)+len(compass)\n",
    "    smooth_steps = 3\n",
    "    hidden_layers = [120,50]\n",
    "else:\n",
    "    raise ValueError('Unknown model')\n",
    "    \n",
    "\n",
    "if create_model:\n",
    "    if not os.path.exists(model_path):\n",
    "        load_dnn = False\n",
    "        os.makedirs(model_path)\n",
    "    else:\n",
    "        load_dnn = True\n",
    "\n",
    "classes = len(labels)\n",
    "\n",
    "hidden_layers.insert(0,(50-(smooth_steps*2))*input_dim)\n",
    "hidden_layers.append(classes)\n",
    "\n",
    "#percentage used for test data\n",
    "test_rate = 0.1\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load samples\n",
    "if load_samples:\n",
    "    samples = ls.load(filename)\n",
    "else:\n",
    "    samples = ls.Samples(labels)\n",
    "\n",
    "    for label in labels:\n",
    "        samples.load_samples(label = label, sample_ids = sample_ids)\n",
    "        \n",
    "    samples.convert_to_input(size = None, accel=accel, gyro=gyro, compass=compass)\n",
    "    \n",
    "    ls.save(samples, filename)\n",
    "    \n",
    "samples.unroll(steps=50) #fixed for angle samples at 50 values\n",
    "samples.convert_to_onehot()\n",
    "for i in range(smooth_steps): samples.smooth()\n",
    "samples.split_test(test_rate)\n",
    "samples.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create graph\n",
    "if not load_dnn:\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    xtest,ytest,_ = samples.get_test(padding=False)\n",
    "    xtest = np.array(xtest)\n",
    "    ytest = np.array(ytest)\n",
    "\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, hidden_layers[0]], name='x')\n",
    "    y = tf.placeholder(tf.int64, [None, classes], name='y')\n",
    "\n",
    "    Ws = []\n",
    "    for i in range(len(hidden_layers)-1):\n",
    "        W = tf.Variable(tf.random_normal([hidden_layers[i], hidden_layers[i+1]]))\n",
    "        b = tf.Variable(tf.zeros(hidden_layers[i+1]))\n",
    "        Ws.append((W,b))\n",
    "\n",
    "\n",
    "    preactivations = []\n",
    "    activations = []\n",
    "    act = x\n",
    "    for W,b in Ws:\n",
    "        preact = tf.matmul(act, W) + b\n",
    "        preactivations.append(preact)\n",
    "        act = tf.sigmoid(preact)\n",
    "        activations.append(act)\n",
    "    dnn = tf.nn.softmax(preact)\n",
    "    dnn_pred = tf.argmax(dnn, 1)\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=preact))\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), dnn_pred)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save and load graph\n",
    "sess = tf.Session()\n",
    "\n",
    "if load_dnn:\n",
    "    saver = tf.train.import_meta_graph('%s/dnn-%s.meta' % (model_path,model_id))\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    optimizer = tf.get_collection('optimizer')[0]\n",
    "    accuracy = tf.get_collection('accuracy')[0]\n",
    "    dnn_pred = tf.get_collection('dnn_pred')[0]\n",
    "    dnn = tf.get_collection('dnn')[0]\n",
    "    merged = tf.get_collection('merged')[0]\n",
    "    x = tf.get_collection('x')[0]\n",
    "    y = tf.get_collection('y')[0]\n",
    "    train_writer = tf.summary.FileWriter('tmp/dnn/summary-%s' % model_id, sess.graph)\n",
    "else:\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    #summaries\n",
    "    def var_summary(var, name):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar(name + '_mean', mean)\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar(name + '_stddev', stddev)\n",
    "        tf.summary.scalar(name + '_max', tf.reduce_max(var))\n",
    "        tf.summary.scalar(name + '_min', tf.reduce_min(var))\n",
    "        tf.summary.histogram(name + '_histogram', var)\n",
    "    i = 0\n",
    "    for W,b in Ws: \n",
    "        var_summary(W, 'weights_%s' % i)\n",
    "        var_summary(b, 'bias_%s' % i)\n",
    "        i += 1\n",
    "    for i in range(len(activations)):\n",
    "        tf.summary.histogram('preactivations_%s' % i, preactivations[i])\n",
    "        tf.summary.histogram('activations_%s' % i, activations[i])\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('tmp/dnn/summary-%s' % model_id, sess.graph)\n",
    "\n",
    "    #init or load from checkpoint\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    tf.add_to_collection('optimizer', optimizer)\n",
    "    tf.add_to_collection('accuracy', accuracy)\n",
    "    tf.add_to_collection('dnn_pred', dnn_pred)\n",
    "    tf.add_to_collection('merged', merged)\n",
    "    tf.add_to_collection('x', x)\n",
    "    tf.add_to_collection('y', y)\n",
    "    tf.add_to_collection('dnn', dnn)\n",
    "    saver.save(sess, '%s/dnn-%s' % (model_path,model_id))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert label class to angle floats\n",
    "\n",
    "def test_all():\n",
    "    xtest,ytest,_ = samples.get_all(use_labels_data=False)\n",
    "    acc = sess.run(accuracy, feed_dict={x: xtest, y: ytest})\n",
    "    print(\"Accuracy train: %s\" % acc)\n",
    "    return acc\n",
    "    \n",
    "def test():\n",
    "    xtest,ytest,_ = samples.get_test(use_labels_data=False)\n",
    "    acc = sess.run(accuracy, feed_dict={x: xtest, y: ytest})\n",
    "    print(\"Accuracy test: %s\" % acc)\n",
    "    return acc\n",
    "    \n",
    "def test_total():\n",
    "    acc1 = test_all()\n",
    "    acc2 = test()\n",
    "    acc = acc1*(1.0-test_rate) + acc2*test_rate\n",
    "    \n",
    "    print('Accuracy total: %s' % acc)\n",
    "    return acc\n",
    "\n",
    "def train(steps, batch_size):\n",
    "    global step_counter\n",
    "    progress = 5\n",
    "    for i in range(steps):\n",
    "        xtrain,ytrain,_ = samples.get_batch(batch_size, use_labels_data=False)\n",
    "        summary,_ = sess.run([merged, optimizer], feed_dict={x: xtrain, y: ytrain})\n",
    "        train_writer.add_summary(summary, i+step_counter)\n",
    "        if i>=progress/100.0*steps-1:\n",
    "            print(\"Progress: %s%%\" % progress)\n",
    "            progress += 5\n",
    "            test()\n",
    "            saver.save(sess, '%s/dnn-%s' % (model_path,model_id), global_step=step_counter)\n",
    "    step_counter += steps\n",
    "    saver.save(sess, '%s/dnn-%s' % (model_path,model_id), global_step=step_counter)\n",
    "    \n",
    "def per_label_acc(test=True):\n",
    "    if test:\n",
    "        xtest,ytest,seq = samples.get_test(use_labels_data=False)\n",
    "    else:\n",
    "        xtest,ytest,seq = samples.get_all(use_labels_data=False)\n",
    "    accs = [0] * classes\n",
    "    counts = [0] * classes\n",
    "\n",
    "    for i in range(len(ytest)):\n",
    "        acc = sess.run(accuracy, feed_dict={x: [xtest[i]], y: [ytest[i]]})\n",
    "        c = 0\n",
    "        for k in range(len(ytest[i])):\n",
    "            if ytest[i][k] > 0.5:\n",
    "                c = k\n",
    "                break\n",
    "        accs[c] += acc\n",
    "        counts[c] += 1\n",
    "\n",
    "    for i in range(classes):\n",
    "        acc = accs[i]/counts[i]\n",
    "        print(counts[i])\n",
    "        print('%s: %s' % (labels[i],acc))\n",
    "        \n",
    "def predict(x_input):\n",
    "    return sess.run(dnn_pred, feed_dict={x:x_input})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    for key in samples.data:\n",
    "        print('%s: %s' % (key, len(samples.data[key])))\n",
    "        #print('%s[0]: %s' % (key, len(samples.data[key][0])))\n",
    "        \n",
    "    xtrain,ytrain,_ = samples.get_batch(10, use_labels_data=False)\n",
    "    res = sess.run([dnn], feed_dict={x: [xtrain[0]], y: [ytrain[0]]})\n",
    "    print(res)\n",
    "    print(sum(res[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    train(10000,40)\n",
    "    test_total()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    per_label_acc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
